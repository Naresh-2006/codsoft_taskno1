# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oD2JHhaO11_L8Ke9SSD5iwtMFsNSF9SJ
"""

# Step 1: Install dependencies
!pip install streamlit pyngrok pandas numpy scikit-learn nltk joblib --quiet

# Step 2: Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re
import string
import joblib
from pyngrok import ngrok

# Step 3: Ensure NLTK data is available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Step 4: Upload and load the dataset
from google.colab import files
print("üìÇ Please upload spam.csv")
uploaded = files.upload()
df = pd.read_csv('spam.csv', encoding='latin-1')

# Step 5: Data Preprocessing
df = df.rename(columns={'v1': 'label', 'v2': 'message'})
df = df[['label', 'message']]
df = df.dropna(subset=['message', 'label'])
df['label'] = df['label'].str.lower().map({'ham': 0, 'spam': 1})
df = df.dropna(subset=['label'])

# Text cleaning function
def preprocess_text(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(token) for token in tokens]
    return ' '.join(tokens)

# Apply preprocessing
df['cleaned_message'] = df['message'].apply(preprocess_text)

# Step 6: Feature Extraction and Model Training
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['cleaned_message']).toarray()
y = df['label'].astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Use class_weight to fix class imbalance
model = LogisticRegression(max_iter=1000, class_weight='balanced')
model.fit(X_train, y_train)

# Evaluate performance
y_pred = model.predict(X_test)
print("üìä Classification Report:\n")
print(classification_report(y_test, y_pred))

# Step 7: Save model and vectorizer
joblib.dump(model, 'sms_spam_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

# Step 8: Write Streamlit app code
streamlit_code = """
import streamlit as st
import joblib
import re
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import string
import pandas as pd

# Ensure NLTK data is available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Load model and vectorizer
model = joblib.load('sms_spam_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

# Text preprocessing function
def preprocess_text(text):
    if not text or isinstance(text, float) and pd.isna(text):
        return ""
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(token) for token in tokens]
    return ' '.join(tokens)

# Streamlit UI
st.title("üì© SMS Spam Classifier")
st.write("Enter an SMS message to classify it as **Spam** or **Ham** (Legitimate).")

user_input = st.text_area("Enter SMS Message", "")
if st.button("Predict"):
    if user_input:
        cleaned_input = preprocess_text(user_input)
        X_input = vectorizer.transform([cleaned_input]).toarray()
        prediction = model.predict(X_input)[0]
        probability = model.predict_proba(X_input)[0][prediction]
        result = "üö´ Spam" if prediction == 1 else "‚úÖ Ham"
        st.markdown(f"**Prediction:** {result}")
        st.markdown(f"**Confidence:** {probability:.2f}")
    else:
        st.error("Please enter a valid SMS message.")
"""

# Save Streamlit app to file
with open('app.py', 'w') as f:
    f.write(streamlit_code)

# Step 9: Set up ngrok and run Streamlit
NGROK_AUTH_TOKEN = "30xiA7nN0U4pZWNh7HSmcHt3HxK_2VG94e3eTAoUsuKGneMFy"  # Replace if needed
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
public_url = ngrok.connect(8501, bind_tls=True)
print(f"üåê Streamlit app is running at: {public_url}")
!streamlit run app.py --server.port 8501 --server.headless true

from google.colab import drive
drive.mount('/content/drive')